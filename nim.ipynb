{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import statistics\n",
    "import itertools\n",
    "import numpy as np\n",
    "import random"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "class Nim():\n",
    "\n",
    "    def __init__(self, n_rows: int = 4):\n",
    "        self.initial_state = [int((x + 1)) for x in range(0, n_rows * 2, 2)]\n",
    "        self.possible_values_in_rows = []\n",
    "        for idx in self.initial_state:\n",
    "            temp_list = []\n",
    "            for ldx in range(idx+1):\n",
    "                temp_list.append(ldx)\n",
    "            self.possible_values_in_rows.append(temp_list)\n",
    "\n",
    "        self.states = (set(itertools.product(*self.possible_values_in_rows)))\n",
    "        self.n_states = len(self.states)\n",
    "        self.current_state = self.initial_state\n",
    "        self.transition_probs = dict()\n",
    "\n",
    "    def fill_transition_probs(self):\n",
    "        for state in self.states:\n",
    "            self.transition_probs[state] = dict()\n",
    "            actions = self.get_possible_actions(state)\n",
    "            for action in actions:\n",
    "                self.transition_probs[state][action] = dict()\n",
    "                new_state = tuple([idx_1 - idx_2 for idx_1, idx_2 in zip(state, action)])\n",
    "                self.transition_probs[state][action][new_state] = 1\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_state = self.initial_state\n",
    "        return self.current_state\n",
    "\n",
    "    def get_all_states(self):\n",
    "        return self.states\n",
    "\n",
    "    def is_terminal(self, state):\n",
    "        if not any(state): return True\n",
    "        return False\n",
    "\n",
    "    def get_possible_actions(self, state):\n",
    "        possible_actions = []\n",
    "\n",
    "        if self.is_terminal(state):\n",
    "            possible_actions.append(state)\n",
    "            return tuple(possible_actions)\n",
    "\n",
    "        for row_idx, number_in_row in enumerate(state):\n",
    "            for number in range(1, number_in_row + 1):\n",
    "                single_action = [0 for _ in range(len(state))]\n",
    "                single_action[row_idx] = number\n",
    "                single_action = tuple(single_action)\n",
    "                possible_actions.append(single_action)\n",
    "\n",
    "        return tuple(possible_actions)\n",
    "\n",
    "    def get_next_states(self, state, action):\n",
    "        assert action in self.get_possible_actions(\n",
    "            state), \"cannot do action %s from state %s\" % (action, state)\n",
    "        return self.transition_probs[state][action]\n",
    "\n",
    "    def get_number_of_states(self):\n",
    "        return self.n_states\n",
    "\n",
    "    def get_reward(self, state, action, next_state):\n",
    "        assert action in self.get_possible_actions(\n",
    "            state), \"cannot do action %s from state %s\" % (action, state)\n",
    "\n",
    "        reward = 1\n",
    "        if self.is_terminal(next_state):\n",
    "            reward = -100\n",
    "\n",
    "        return reward\n",
    "\n",
    "    def step(self, action):\n",
    "        prev_state = self.current_state\n",
    "        self.current_state = [idx_1 - idx_2 for idx_1, idx_2 in zip(self.current_state, action)]\n",
    "        return self.current_state, self.get_reward(prev_state, action, self.current_state), \\\n",
    "               self.is_terminal(self.current_state), None"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50319 49681 100000\n"
     ]
    }
   ],
   "source": [
    "# play at random\n",
    "nim = Nim()\n",
    "player_1_wins = 0\n",
    "player_2_wins = 0\n",
    "games = 0\n",
    "\n",
    "for _ in range(100000):\n",
    "    nim.reset()\n",
    "    turn = 0\n",
    "    while not nim.is_terminal(nim.current_state):\n",
    "        random_action = random.choice(nim.get_possible_actions(nim.current_state))\n",
    "        nim.step(random_action)\n",
    "        if nim.is_terminal(nim.current_state):\n",
    "            games += 1\n",
    "            if turn % 2:\n",
    "                player_2_wins += 1\n",
    "            else:\n",
    "                player_1_wins += 1\n",
    "        turn += 1\n",
    "\n",
    "print(player_1_wins, player_2_wins, games)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "UCZENIE PASYWNE"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "def value_iteration(nim, gamma, theta):\n",
    "    V = dict()\n",
    "    nim.fill_transition_probs()\n",
    "\n",
    "    for state in nim.get_all_states():\n",
    "        V[state] = 0\n",
    "\n",
    "    policy = dict()\n",
    "    for current_state in nim.get_all_states():\n",
    "        try:\n",
    "            policy[current_state] = nim.get_possible_actions(current_state)[0]\n",
    "        except IndexError:\n",
    "            continue\n",
    "\n",
    "    while True:\n",
    "        last_mean_value = statistics.fmean(V.values())\n",
    "        for current_state in nim.get_all_states():\n",
    "            actions = nim.get_possible_actions(current_state)\n",
    "            state_action_values = dict()\n",
    "\n",
    "            for action in actions:\n",
    "\n",
    "                state_action_values[action] = 0\n",
    "                next_states = nim.get_next_states(current_state, action)\n",
    "\n",
    "                for next_state in next_states:\n",
    "                    state_action_values[action] += next_states[next_state] * (nim.get_reward(current_state, action, next_state) + gamma * V[next_state])\n",
    "\n",
    "            V[current_state] = max(list(state_action_values.values()))\n",
    "        if abs(statistics.fmean(V.values()) - last_mean_value) < theta:\n",
    "            break\n",
    "\n",
    "    for current_state in nim.get_all_states():\n",
    "\n",
    "        state_action_values = dict()\n",
    "        actions = nim.get_possible_actions(current_state)\n",
    "\n",
    "        for action in actions:\n",
    "            state_action_values[action] = 0\n",
    "            next_states = nim.get_next_states(current_state, action)\n",
    "\n",
    "            for next_state in next_states:\n",
    "                state_action_values[action] += next_states[next_state] * (nim.get_reward(current_state, action, next_state) + gamma * V[next_state])\n",
    "\n",
    "        max_value_action = max(state_action_values, key=state_action_values.get)\n",
    "\n",
    "        if policy[current_state] != max_value_action:\n",
    "            policy[current_state] = max_value_action\n",
    "\n",
    "    return policy, V"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "nim = Nim(4)\n",
    "optimal_policy, optimal_value = value_iteration(nim, 0.9, 0.001)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7076 2924 10000\n"
     ]
    }
   ],
   "source": [
    "# play (player 1) value iteration vs random\n",
    "player_1_wins = 0\n",
    "player_2_wins = 0\n",
    "games = 0\n",
    "\n",
    "for _ in range(10000):\n",
    "    nim.reset()\n",
    "    turn = 0\n",
    "    while not nim.is_terminal(nim.current_state):\n",
    "        if not turn % 2:\n",
    "            action = optimal_policy[tuple(nim.current_state)]\n",
    "        else:\n",
    "            action = random.choice(nim.get_possible_actions(nim.current_state))\n",
    "        nim.step(action)\n",
    "        if nim.is_terminal(nim.current_state):\n",
    "            games += 1\n",
    "            if turn % 2:\n",
    "                player_2_wins += 1\n",
    "            else:\n",
    "                player_1_wins += 1\n",
    "        turn += 1\n",
    "\n",
    "print(player_1_wins, player_2_wins, games)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "UCZENIE AKTYWNE"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "class SARSALambdaAgent:\n",
    "    def __init__(self, alpha, epsilon, discount, get_legal_actions, lambda_value):\n",
    "        \"\"\"\n",
    "        SARSA Lambda Agent\n",
    "        based on https://inst.eecs.berkeley.edu/~cs188/sp19/projects.html\n",
    "        Instance variables you have access to\n",
    "          - self.epsilon (exploration prob)\n",
    "          - self.alpha (learning rate)\n",
    "          - self.discount (discount rate aka gamma)\n",
    "\n",
    "        Functions you should use\n",
    "          - self.get_legal_actions(state) {state, hashable -> list of actions, each is hashable}\n",
    "            which returns legal actions for a state\n",
    "          - self.get_qvalue(state,action)\n",
    "            which returns Q(state,action)\n",
    "          - self.set_qvalue(state,action,value)\n",
    "            which sets Q(state,action) := value\n",
    "        !!!Important!!!\n",
    "        Note: please avoid using self._qValues directly.\n",
    "            There's a special self.get_qvalue/set_qvalue for that.\n",
    "        \"\"\"\n",
    "\n",
    "        self.get_legal_actions = get_legal_actions\n",
    "        self._qvalues = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "        self._evalues = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.discount = discount\n",
    "        self.lambda_value = lambda_value\n",
    "\n",
    "    def get_qvalue(self, state, action):\n",
    "        \"\"\" Returns Q(state,action) \"\"\"\n",
    "        return self._qvalues[str(state)][str(action)]\n",
    "\n",
    "    def set_qvalue(self, state, action, value):\n",
    "        \"\"\" Sets the Qvalue for [state,action] to the given value \"\"\"\n",
    "        self._qvalues[str(state)][str(action)] = value\n",
    "\n",
    "    def reset(self):\n",
    "        self._evalues = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "\n",
    "    # ---------------------START OF YOUR CODE---------------------#\n",
    "\n",
    "    def get_value(self, state):\n",
    "        \"\"\"\n",
    "        Compute your agent's estimate of V(s) using current q-values\n",
    "        V(s) = max_over_action Q(state,action) over possible actions.\n",
    "        Note: please take into account that q-values can be negative.\n",
    "        \"\"\"\n",
    "        possible_actions = self.get_legal_actions(state)\n",
    "\n",
    "        if len(possible_actions) == 0:\n",
    "            return 0.0\n",
    "\n",
    "        return max([self.get_qvalue(state, action) for action in possible_actions])\n",
    "\n",
    "    def update(self, state, action, reward, next_state):\n",
    "        \"\"\"\n",
    "        You should do your SARSA-Lambda update here:\n",
    "        \"\"\"\n",
    "\n",
    "        # agent parameters\n",
    "        gamma = self.discount\n",
    "        learning_rate = self.alpha\n",
    "\n",
    "        next_action = self.get_action(next_state)\n",
    "        delta = reward + gamma * self.get_qvalue(next_state, next_action) - self.get_qvalue(state, action)\n",
    "        self._evalues[str(state)][str(action)] += (1 - learning_rate) * gamma * self.lambda_value * self._evalues[str(state)][str(action)] + 1\n",
    "\n",
    "        for every_state in self._qvalues.keys():\n",
    "            for every_action in self._qvalues[every_state].keys():\n",
    "                qvalue = self.get_qvalue(every_state, every_action) + learning_rate * delta * self._evalues[every_state][every_action]\n",
    "                self.set_qvalue(every_state, every_action, qvalue)\n",
    "                self._evalues[every_state][every_action] = gamma * self.lambda_value * self._evalues[every_state][every_action]\n",
    "\n",
    "        return next_action\n",
    "\n",
    "    def get_best_action(self, state):\n",
    "        \"\"\"\n",
    "        Compute the best action to take in a state (using current q-values).\n",
    "        \"\"\"\n",
    "        possible_actions = self.get_legal_actions(state)\n",
    "\n",
    "        # If there are no legal actions, return None\n",
    "        if len(possible_actions) == 0:\n",
    "            return None\n",
    "\n",
    "        possible_actions_dict = dict()\n",
    "        for action in possible_actions:\n",
    "            possible_actions_dict[tuple(action)] = self.get_qvalue(state, action)\n",
    "        sorted_dict = sorted(possible_actions_dict.items(), key=lambda kv: kv[1])\n",
    "\n",
    "        return random.choice([k for k, v in possible_actions_dict.items() if v == sorted_dict[-1][-1]])\n",
    "\n",
    "    def get_action(self, state):\n",
    "        \"\"\"\n",
    "        Compute the action to take in the current state, including exploration.\n",
    "        With probability self.epsilon, we should take a random action.\n",
    "            otherwise - the best policy action (self.get_best_action).\n",
    "\n",
    "        Note: To pick randomly from a list, use random.choice(list).\n",
    "              To pick True or False with a given probablity, generate uniform number in [0, 1]\n",
    "              and compare it with your probability\n",
    "        \"\"\"\n",
    "\n",
    "        # Pick Action\n",
    "        possible_actions = self.get_legal_actions(state)\n",
    "\n",
    "        # If there are no legal actions, return None\n",
    "        if len(possible_actions) == 0:\n",
    "            return None\n",
    "\n",
    "        # agent parameters:\n",
    "        epsilon = self.epsilon\n",
    "\n",
    "        if random.random() < epsilon:\n",
    "            return random.choice(possible_actions)\n",
    "\n",
    "        return self.get_best_action(state)\n",
    "\n",
    "    def turn_off_learning(self):\n",
    "        self.epsilon = 0\n",
    "        self.alpha = 0\n",
    "\n",
    "    def display_qvalues(self):\n",
    "        for s in self._qvalues:\n",
    "            print(\"State: \" + str(s) + \" \" + str(self._qvalues[s]))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class ExpectedSARSAAgent:\n",
    "    def __init__(self, alpha, epsilon, discount, get_legal_actions):\n",
    "        \"\"\"\n",
    "        Q-Learning Agent\n",
    "        based on https://inst.eecs.berkeley.edu/~cs188/sp19/projects.html\n",
    "        Instance variables you have access to\n",
    "          - self.epsilon (exploration prob)\n",
    "          - self.alpha (learning rate)\n",
    "          - self.discount (discount rate aka gamma)\n",
    "\n",
    "        Functions you should use\n",
    "          - self.get_legal_actions(state) {state, hashable -> list of actions, each is hashable}\n",
    "            which returns legal actions for a state\n",
    "          - self.get_qvalue(state,action)\n",
    "            which returns Q(state,action)\n",
    "          - self.set_qvalue(state,action,value)\n",
    "            which sets Q(state,action) := value\n",
    "        !!!Important!!!\n",
    "        Note: please avoid using self._qValues directly.\n",
    "            There's a special self.get_qvalue/set_qvalue for that.\n",
    "        \"\"\"\n",
    "\n",
    "        self.get_legal_actions = get_legal_actions\n",
    "        self._qvalues = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.discount = discount\n",
    "\n",
    "    def get_qvalue(self, state, action):\n",
    "        \"\"\" Returns Q(state,action) \"\"\"\n",
    "        return self._qvalues[str(state)][str(action)]\n",
    "\n",
    "    def set_qvalue(self, state, action, value):\n",
    "        \"\"\" Sets the Qvalue for [state,action] to the given value \"\"\"\n",
    "        self._qvalues[str(state)][str(action)] = value\n",
    "\n",
    "    #---------------------START OF YOUR CODE---------------------#\n",
    "\n",
    "    def get_value(self, state):\n",
    "        \"\"\"\n",
    "        Compute your agent's estimate of V(s) using current q-values\n",
    "        V(s) = max_over_action Q(state,action) over possible actions.\n",
    "        Note: please take into account that q-values can be negative.\n",
    "        \"\"\"\n",
    "        possible_actions = self.get_legal_actions(state)\n",
    "\n",
    "        # If there are no legal actions, return 0.0\n",
    "        if len(possible_actions) == 0:\n",
    "            return 0.0\n",
    "\n",
    "        #\n",
    "        # INSERT CODE HERE to get maximum possible value for a given state\n",
    "        #\n",
    "\n",
    "        return max([self.get_qvalue(state, action) for action in possible_actions])\n",
    "\n",
    "    def update(self, state, action, reward, next_state):\n",
    "        \"\"\"\n",
    "        You should do your Q-Value update here:\n",
    "           Q(s,a) := (1 - alpha) * Q(s,a) + alpha * (r + gamma * \\sum_a \\pi(a|s') Q(s', a))\n",
    "        \"\"\"\n",
    "\n",
    "        # agent parameters\n",
    "        gamma = self.discount\n",
    "        learning_rate = self.alpha\n",
    "\n",
    "        sum_of_strategies = list()\n",
    "\n",
    "        [sum_of_strategies.append(1 / len(self.get_legal_actions(next_state)) * self.get_qvalue(next_state, action)) for action in self.get_legal_actions(next_state)]\n",
    "\n",
    "        value = (1 - learning_rate) * self.get_qvalue(state, action) + learning_rate * (reward + gamma * sum(sum_of_strategies))\n",
    "        self.set_qvalue(state, action, value)\n",
    "\n",
    "    def get_best_action(self, state):\n",
    "        \"\"\"\n",
    "        Compute the best action to take in a state (using current q-values).\n",
    "        \"\"\"\n",
    "        possible_actions = self.get_legal_actions(state)\n",
    "\n",
    "        # If there are no legal actions, return None\n",
    "        if len(possible_actions) == 0:\n",
    "            return None\n",
    "\n",
    "        possible_actions_dict = dict()\n",
    "\n",
    "        for action in possible_actions:\n",
    "            possible_actions_dict[tuple(action)] = self.get_qvalue(state, action)\n",
    "\n",
    "        sorted_dict = sorted(possible_actions_dict.items(), key=lambda kv: kv[1])\n",
    "\n",
    "        return random.choice([k for k, v in possible_actions_dict.items() if v == sorted_dict[-1][-1]])\n",
    "\n",
    "    def get_action(self, state):\n",
    "        \"\"\"\n",
    "        Compute the action to take in the current state, including exploration.\n",
    "        With probability self.epsilon, we should take a random action.\n",
    "            otherwise - the best policy action (self.get_best_action).\n",
    "\n",
    "        Note: To pick randomly from a list, use random.choice(list).\n",
    "              To pick True or False with a given probablity, generate uniform number in [0, 1]\n",
    "              and compare it with your probability\n",
    "        \"\"\"\n",
    "\n",
    "        # Pick Action\n",
    "        possible_actions = self.get_legal_actions(state)\n",
    "\n",
    "        # If there are no legal actions, return None\n",
    "        if len(possible_actions) == 0:\n",
    "            return None\n",
    "\n",
    "        # agent parameters:\n",
    "        epsilon = self.epsilon\n",
    "\n",
    "        if random.random() < epsilon:\n",
    "            return random.choice(possible_actions)\n",
    "\n",
    "        return self.get_best_action(state)\n",
    "\n",
    "    def turn_off_learning(self):\n",
    "        \"\"\"\n",
    "        Function turns off agent learning.\n",
    "        \"\"\"\n",
    "        self.epsilon = 0\n",
    "        self.alpha = 0\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "def play_and_train_exp(env, agent):\n",
    "    \"\"\"\n",
    "    This function should\n",
    "    - run a full game, actions given by agent's e-greedy policy\n",
    "    - train agent using agent.update(...) whenever it is possible\n",
    "    - return total reward\n",
    "    \"\"\"\n",
    "    total_reward = 0.0\n",
    "    state = env.reset()\n",
    "\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        # get agent to pick action given state state.\n",
    "        action = agent.get_action(state)\n",
    "\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        agent.update(state, action, reward, next_state)\n",
    "\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return total_reward"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "def play_and_train_lambda(env, agent):\n",
    "    \"\"\"\n",
    "    This function should\n",
    "    - run a full game, actions given by agent's e-greedy policy\n",
    "    - train agent using agent.update(...) whenever it is possible\n",
    "    - return total reward\n",
    "    \"\"\"\n",
    "    total_reward = 0.0\n",
    "    state = env.reset()\n",
    "\n",
    "    done = False\n",
    "\n",
    "    agent.reset()\n",
    "    action = agent.get_action(state)\n",
    "\n",
    "    while not done:\n",
    "        # get agent to pick action given state state.\n",
    "\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        action = agent.update(state, action, reward, next_state)\n",
    "\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return total_reward"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "nim = Nim()\n",
    "agent_lambda = SARSALambdaAgent(alpha=0.1, epsilon=0.1, discount=0.99,\n",
    "                   get_legal_actions=nim.get_possible_actions, lambda_value = 0.5)\n",
    "\n",
    "agent_exp = ExpectedSARSAAgent(alpha=0.1, epsilon=0.1, discount=0.99,\n",
    "                       get_legal_actions=nim.get_possible_actions)\n",
    "\n",
    "for i in range(10000):\n",
    "    play_and_train_exp(nim, agent_exp)\n",
    "    play_and_train_lambda(nim, agent_lambda)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61261 38739 100000\n"
     ]
    }
   ],
   "source": [
    "# play sarsa lambda vs random\n",
    "player_1_wins = 0\n",
    "player_2_wins = 0\n",
    "games = 0\n",
    "\n",
    "for _ in range(100000):\n",
    "    nim.reset()\n",
    "    turn = 0\n",
    "    while not nim.is_terminal(nim.current_state):\n",
    "        if not turn % 2:\n",
    "            # action = agent_exp.get_best_action(nim.current_state)\n",
    "            action = agent_lambda.get_best_action(nim.current_state)\n",
    "        else:\n",
    "            action = random.choice(nim.get_possible_actions(nim.current_state))\n",
    "\n",
    "        nim.step(action)\n",
    "\n",
    "        if nim.is_terminal(nim.current_state):\n",
    "            games += 1\n",
    "            if turn % 2:\n",
    "                player_2_wins += 1\n",
    "            else:\n",
    "                player_1_wins += 1\n",
    "\n",
    "        turn += 1\n",
    "\n",
    "print(player_1_wins, player_2_wins, games)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "MCTS"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MonteCarloTreeSearchNode():\n",
    "    def __init__(self, state, parent=None, parent_action=None):\n",
    "        self.state = state\n",
    "        self.parent = parent\n",
    "        self.parent_action = parent_action\n",
    "        self.children = []\n",
    "        self._number_of_visits = 0\n",
    "        self._results = defaultdict(int)\n",
    "        self._results[1] = 0\n",
    "        self._results[-1] = 0\n",
    "        self._untried_actions = None\n",
    "        return\n",
    "\n",
    "    def untried_actions(self):\n",
    "        self._untried_actions = self.state.get_legal_actions()\n",
    "        return self._untried_actions\n",
    "\n",
    "    def q(self):\n",
    "        wins = self._results[1]\n",
    "        loses = self._results[-1]\n",
    "        return wins - loses\n",
    "\n",
    "    def n(self):\n",
    "        return self._number_of_visits\n",
    "\n",
    "    def expand(self):\n",
    "\n",
    "        action = self._untried_actions.pop()\n",
    "        next_state = self.state.move(action)\n",
    "        child_node = MonteCarloTreeSearchNode(\n",
    "            next_state, parent=self, parent_action=action)\n",
    "\n",
    "        self.children.append(child_node)\n",
    "        return child_node\n",
    "\n",
    "    def is_terminal_node(self):\n",
    "        return self.state.is_game_over()\n",
    "\n",
    "    def rollout(self):\n",
    "        current_rollout_state = self.state\n",
    "\n",
    "        while not current_rollout_state.is_game_over():\n",
    "\n",
    "            possible_moves = current_rollout_state.get_legal_actions()\n",
    "\n",
    "            action = self.rollout_policy(possible_moves)\n",
    "            current_rollout_state = current_rollout_state.move(action)\n",
    "        return current_rollout_state.game_result()\n",
    "\n",
    "    def backpropagate(self, result):\n",
    "        self._number_of_visits += 1.\n",
    "        self._results[result] += 1.\n",
    "        if self.parent:\n",
    "            self.parent.backpropagate(result)\n",
    "\n",
    "    def is_fully_expanded(self):\n",
    "        return len(self._untried_actions) == 0\n",
    "\n",
    "    def best_child(self, c_param=0.1):\n",
    "\n",
    "        choices_weights = [(c.q() / c.n()) + c_param * np.sqrt((2 * np.log(self.n()) / c.n())) for c in self.children]\n",
    "        return self.children[np.argmax(choices_weights)]\n",
    "\n",
    "    def rollout_policy(self, possible_moves):\n",
    "\n",
    "        return possible_moves[np.random.randint(len(possible_moves))]\n",
    "\n",
    "    def _tree_policy(self):\n",
    "\n",
    "        current_node = self\n",
    "        while not current_node.is_terminal_node():\n",
    "\n",
    "            if not current_node.is_fully_expanded():\n",
    "                return current_node.expand()\n",
    "            else:\n",
    "                current_node = current_node.best_child()\n",
    "        return current_node\n",
    "\n",
    "    def best_action(self):\n",
    "        simulation_no = 100\n",
    "\n",
    "        for i in range(simulation_no):\n",
    "\n",
    "            v = self._tree_policy()\n",
    "            reward = v.rollout()\n",
    "            v.backpropagate(reward)\n",
    "\n",
    "        return self.best_child(c_param=0.)\n",
    "\n",
    "    def get_legal_actions(self):\n",
    "        possible_actions = []\n",
    "\n",
    "        # if self.is_terminal(self.state):\n",
    "        #     possible_actions.append(state)\n",
    "        #     return tuple(possible_actions)\n",
    "\n",
    "        for row_idx, number_in_row in enumerate(self.state):\n",
    "            for number in range(1, number_in_row + 1):\n",
    "                single_action = [0 for _ in range(len(self.state))]\n",
    "                single_action[row_idx] = number\n",
    "                single_action = tuple(single_action)\n",
    "                possible_actions.append(single_action)\n",
    "\n",
    "        return tuple(possible_actions)\n",
    "\n",
    "    def is_game_over(self):\n",
    "        if not any(self.state): return True\n",
    "        return False\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "class Nim():\n",
    "\n",
    "    def __init__(self, n_rows: int = 4):\n",
    "        self.initial_state = [int((x + 1)) for x in range(0, n_rows * 2, 2)]\n",
    "        self.possible_values_in_rows = []\n",
    "        for idx in self.initial_state:\n",
    "            temp_list = []\n",
    "            for ldx in range(idx+1):\n",
    "                temp_list.append(ldx)\n",
    "            self.possible_values_in_rows.append(temp_list)\n",
    "\n",
    "        self.states = (set(itertools.product(*self.possible_values_in_rows)))\n",
    "        self.n_states = len(self.states)\n",
    "        self.current_state = self.initial_state\n",
    "        self.transition_probs = dict()\n",
    "\n",
    "    def fill_transition_probs(self):\n",
    "        for state in self.states:\n",
    "            self.transition_probs[state] = dict()\n",
    "            actions = self.get_possible_actions(state)\n",
    "            for action in actions:\n",
    "                self.transition_probs[state][action] = dict()\n",
    "                new_state = tuple([idx_1 - idx_2 for idx_1, idx_2 in zip(state, action)])\n",
    "                self.transition_probs[state][action][new_state] = 1\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_state = self.initial_state\n",
    "        return self.current_state\n",
    "\n",
    "    def get_all_states(self):\n",
    "        return self.states\n",
    "\n",
    "    def is_terminal(self, state):\n",
    "        if not any(state): return True\n",
    "        return False\n",
    "\n",
    "    def get_possible_actions(self, state):\n",
    "        possible_actions = []\n",
    "\n",
    "        if self.is_terminal(state):\n",
    "            possible_actions.append(state)\n",
    "            return tuple(possible_actions)\n",
    "\n",
    "        for row_idx, number_in_row in enumerate(state):\n",
    "            for number in range(1, number_in_row + 1):\n",
    "                single_action = [0 for _ in range(len(state))]\n",
    "                single_action[row_idx] = number\n",
    "                single_action = tuple(single_action)\n",
    "                possible_actions.append(single_action)\n",
    "\n",
    "        return tuple(possible_actions)\n",
    "\n",
    "    def get_next_states(self, state, action):\n",
    "        assert action in self.get_possible_actions(\n",
    "            state), \"cannot do action %s from state %s\" % (action, state)\n",
    "        return self.transition_probs[state][action]\n",
    "\n",
    "    def get_number_of_states(self):\n",
    "        return self.n_states\n",
    "\n",
    "    def get_reward(self, state, action, next_state):\n",
    "        assert action in self.get_possible_actions(\n",
    "            state), \"cannot do action %s from state %s\" % (action, state)\n",
    "\n",
    "        reward = -1\n",
    "        if self.is_terminal(next_state):\n",
    "            reward = 100\n",
    "\n",
    "        return reward\n",
    "\n",
    "    def step(self, action):\n",
    "        prev_state = self.current_state\n",
    "        self.current_state = [idx_1 - idx_2 for idx_1, idx_2 in zip(self.current_state, action)]\n",
    "        return self.current_state, self.get_reward(prev_state, action, self.current_state), \\\n",
    "               self.is_terminal(self.current_state), None"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "mcts = MonteCarloTreeSearchNode(state=nim.initial_state)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}